//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24817639
// Cuda compilation tools, release 10.0, V10.0.130
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_30
.address_size 64

	// .globl	mandel_float
.global .align 1 .b8 viridis[768] = {84, 1, 68, 85, 2, 68, 87, 3, 69, 88, 5, 69, 90, 6, 69, 91, 8, 70, 93, 9, 70, 94, 11, 70, 96, 12, 70, 97, 14, 71, 98, 15, 71, 100, 17, 71, 101, 18, 71, 102, 20, 71, 104, 21, 72, 105, 22, 72, 106, 24, 72, 108, 25, 72, 109, 26, 72, 110, 28, 72, 111, 29, 72, 112, 30, 72, 113, 32, 72, 115, 33, 72, 116, 34, 72, 117, 36, 72, 118, 37, 72, 119, 38, 72, 120, 39, 72, 121, 41, 71, 121, 42, 71, 122, 43, 71, 123, 44, 71, 124, 46, 71, 125, 47, 70, 126, 48, 70, 126, 49, 70, 127, 51, 70, 128, 52, 69, 129, 53, 69, 129, 54, 69, 130, 56, 68, 131, 57, 68, 131, 58, 68, 132, 59, 67, 132, 60, 67, 133, 62, 67, 133, 63, 66, 134, 64, 66, 134, 65, 65, 135, 66, 65, 135, 67, 65, 136, 69, 64, 136, 70, 64, 136, 71, 63, 137, 72, 63, 137, 73, 62, 137, 74, 62, 138, 75, 61, 138, 77, 61, 138, 78, 60, 138, 79, 60, 139, 80, 59, 139, 81, 59, 139, 82, 58, 139, 83, 58, 140, 84, 57, 140, 85, 57, 140, 86, 56, 140, 87, 56, 140, 88, 55, 140, 89, 55, 141, 91, 54, 141, 92, 54, 141, 93, 53, 141, 94, 53, 141, 95, 52, 141, 96, 52, 141, 97, 51, 141, 98, 51, 141, 99, 51, 142, 100, 50, 142, 101, 50, 142, 102, 49, 142, 103, 49, 142, 104, 48, 142, 105, 48, 142, 106, 47, 142, 107, 47, 142, 108, 47, 142, 109, 46, 142, 110, 46, 142, 111, 45, 142, 112, 45, 142, 112, 45, 142, 113, 44, 142, 114, 44, 142, 115, 43, 142, 116, 43, 142, 117, 43, 142, 118, 42, 142, 119, 42, 142, 120, 41, 142, 121, 41, 142, 122, 41, 142, 123, 40, 142, 124, 40, 142, 125, 40, 142, 126, 39, 142, 127, 39, 142, 128, 38, 142, 129, 38, 142, 130, 38, 142, 130, 37, 142, 131, 37, 142, 132, 37, 142, 133, 36, 142, 134, 36, 142, 135, 35, 142, 136, 35, 142, 137, 35, 141, 138, 34, 141, 139, 34, 141, 140, 34, 141, 141, 33, 141, 142, 33, 141, 143, 33, 141, 144, 32, 140, 145, 32, 140, 146, 32, 140, 147, 32, 140, 147, 31, 140, 148, 31, 139, 149, 31, 139, 150, 31, 139, 151, 31, 139, 152, 30, 138, 153, 30, 138, 154, 30, 138, 155, 30, 137, 156, 30, 137, 157, 30, 137, 158, 30, 136, 159, 30, 136, 160, 30, 136, 161, 31, 135, 162, 31, 135, 163, 31, 134, 163, 31, 134, 164, 32, 134, 165, 32, 133, 166, 33, 133, 167, 33, 132, 168, 34, 131, 169, 35, 131, 170, 35, 130, 171, 36, 130, 172, 37, 129, 173, 38, 129, 174, 39, 128, 175, 40, 127, 175, 41, 127, 176, 42, 126, 177, 43, 125, 178, 44, 124, 179, 46, 124, 180, 47, 123, 181, 48, 122, 182, 50, 121, 183, 51, 121, 183, 53, 120, 184, 54, 119, 185, 56, 118, 186, 57, 117, 187, 59, 116, 188, 61, 115, 189, 62, 114, 189, 64, 113, 190, 66, 112, 191, 68, 111, 192, 70, 110, 193, 72, 109, 194, 73, 108, 194, 75, 107, 195, 77, 106, 196, 79, 105, 197, 81, 104, 198, 83, 102, 198, 85, 101, 199, 88, 100, 200, 90, 99, 201, 92, 98, 201, 94, 96, 202, 96, 95, 203, 98, 94, 204, 101, 92, 204, 103, 91, 205, 105, 90, 206, 108, 88, 206, 110, 87, 207, 112, 85, 208, 115, 84, 208, 117, 82, 209, 119, 81, 210, 122, 79, 210, 124, 78, 211, 127, 76, 212, 129, 75, 212, 132, 73, 213, 134, 72, 213, 137, 70, 214, 139, 68, 215, 142, 67, 215, 144, 65, 216, 147, 63, 216, 149, 62, 217, 152, 60, 217, 155, 58, 218, 157, 57, 218, 160, 55, 219, 163, 53, 219, 165, 51, 220, 168, 50, 220, 171, 48, 221, 173, 46, 221, 176, 45, 221, 179, 43, 222, 181, 41, 222, 184, 39, 223, 187, 38, 223, 189, 36, 223, 192, 35, 224, 195, 33, 224, 197, 32, 225, 200, 30, 225, 203, 29, 225, 205, 28, 226, 208, 27, 226, 211, 26, 226, 213, 25, 227, 216, 24, 227, 219, 24, 227, 221, 24, 228, 224, 24, 228, 226, 24, 228, 229, 25, 229, 232, 25, 229, 234, 26, 229, 237, 27, 230, 239, 28, 230, 242, 30, 230, 244, 31, 230, 247, 33, 231, 249, 35, 231, 251, 36, 231, 254};

.visible .entry mandel_float(
	.param .u64 mandel_float_param_0,
	.param .u32 mandel_float_param_1,
	.param .u32 mandel_float_param_2,
	.param .u32 mandel_float_param_3,
	.param .u32 mandel_float_param_4,
	.param .f32 mandel_float_param_5,
	.param .f32 mandel_float_param_6,
	.param .f32 mandel_float_param_7,
	.param .u32 mandel_float_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<19>;
	.reg .f32 	%f<38>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [mandel_float_param_0];
	ld.param.u32 	%r6, [mandel_float_param_1];
	ld.param.u32 	%r7, [mandel_float_param_2];
	ld.param.u32 	%r8, [mandel_float_param_3];
	ld.param.u32 	%r9, [mandel_float_param_4];
	ld.param.f32 	%f16, [mandel_float_param_5];
	ld.param.f32 	%f17, [mandel_float_param_6];
	ld.param.f32 	%f18, [mandel_float_param_7];
	ld.param.u32 	%r10, [mandel_float_param_8];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r2, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r11;
	mad.lo.s32 	%r19, %r18, %r2, %r1;
	setp.lt.s32	%p1, %r1, %r8;
	setp.lt.s32	%p2, %r2, %r9;
	and.pred  	%p3, %p1, %p2;
	mul.wide.s32 	%rd4, %r19, 4;
	add.s64 	%rd1, %rd3, %rd4;
	@%p3 bra 	BB0_3;
	bra.uni 	BB0_1;

BB0_3:
	cvt.rn.f32.s32	%f1, %r9;
	rcp.rn.f32 	%f35, %f18;
	mul.f32 	%f19, %f1, %f35;
	cvt.rn.f32.s32	%f3, %r8;
	div.rn.f32 	%f4, %f19, %f3;
	setp.le.s32	%p7, %r9, %r8;
	@%p7 bra 	BB0_4;

	mul.f32 	%f20, %f3, %f35;
	div.rn.f32 	%f34, %f20, %f1;
	bra.uni 	BB0_6;

BB0_1:
	setp.lt.s32	%p4, %r1, %r6;
	setp.lt.s32	%p5, %r2, %r7;
	and.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB0_13;
	bra.uni 	BB0_2;

BB0_2:
	mov.u16 	%rs7, 0;
	mov.u16 	%rs8, 255;
	st.global.v4.u8 	[%rd1], {%rs7, %rs8, %rs8, %rs7};
	bra.uni 	BB0_13;

BB0_4:
	mov.f32 	%f34, %f35;
	mov.f32 	%f35, %f4;

BB0_6:
	add.s32 	%r21, %r8, -1;
	cvt.rn.f32.s32	%f21, %r21;
	add.f32 	%f22, %f34, %f34;
	div.rn.f32 	%f23, %f22, %f21;
	add.s32 	%r22, %r9, -1;
	cvt.rn.f32.s32	%f24, %r22;
	add.f32 	%f25, %f35, %f35;
	div.rn.f32 	%f26, %f25, %f24;
	cvt.rn.f32.s32	%f27, %r1;
	sub.f32 	%f28, %f16, %f34;
	fma.rn.f32 	%f8, %f27, %f23, %f28;
	cvt.rn.f32.s32	%f29, %r2;
	sub.f32 	%f30, %f17, %f35;
	fma.rn.f32 	%f9, %f29, %f26, %f30;
	setp.eq.s32	%p8, %r10, 0;
	mov.u32 	%r24, 0;
	shl.b32 	%r3, %r10, 8;
	mov.u16 	%rs11, 0;
	mov.u16 	%rs16, %rs11;
	mov.u16 	%rs17, %rs11;
	mov.u16 	%rs18, %rs11;
	@%p8 bra 	BB0_12;

	mov.f32 	%f36, %f9;
	mov.f32 	%f37, %f8;

BB0_8:
	mul.f32 	%f12, %f36, %f36;
	mul.f32 	%f13, %f37, %f37;
	add.f32 	%f31, %f13, %f12;
	setp.gt.f32	%p9, %f31, 0f40800000;
	@%p9 bra 	BB0_11;

	add.f32 	%f32, %f37, %f37;
	fma.rn.f32 	%f36, %f32, %f36, %f9;
	sub.f32 	%f33, %f13, %f12;
	add.f32 	%f37, %f8, %f33;
	add.s32 	%r24, %r24, 1;
	setp.lt.u32	%p10, %r24, %r3;
	@%p10 bra 	BB0_8;

	mov.u16 	%rs16, %rs11;
	mov.u16 	%rs17, %rs11;
	mov.u16 	%rs18, %rs11;
	bra.uni 	BB0_12;

BB0_11:
	and.b32  	%r23, %r24, 255;
	mul.wide.u32 	%rd5, %r23, 3;
	mov.u64 	%rd6, viridis;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.u8 	%rs18, [%rd7];
	ld.global.u8 	%rs17, [%rd7+1];
	ld.global.u8 	%rs16, [%rd7+2];

BB0_12:
	st.global.v4.u8 	[%rd1], {%rs18, %rs17, %rs16, %rs11};

BB0_13:
	ret;
}

	// .globl	mandel_double
.visible .entry mandel_double(
	.param .u64 mandel_double_param_0,
	.param .u32 mandel_double_param_1,
	.param .u32 mandel_double_param_2,
	.param .u32 mandel_double_param_3,
	.param .u32 mandel_double_param_4,
	.param .f64 mandel_double_param_5,
	.param .f64 mandel_double_param_6,
	.param .f64 mandel_double_param_7,
	.param .u32 mandel_double_param_8
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [mandel_double_param_0];
	ld.param.u32 	%r6, [mandel_double_param_1];
	ld.param.u32 	%r7, [mandel_double_param_2];
	ld.param.u32 	%r8, [mandel_double_param_3];
	ld.param.u32 	%r9, [mandel_double_param_4];
	ld.param.f64 	%fd16, [mandel_double_param_5];
	ld.param.f64 	%fd17, [mandel_double_param_6];
	ld.param.f64 	%fd18, [mandel_double_param_7];
	ld.param.u32 	%r10, [mandel_double_param_8];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r1, %r11, %r12, %r13;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r2, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r11;
	mad.lo.s32 	%r19, %r18, %r2, %r1;
	setp.lt.s32	%p1, %r1, %r8;
	setp.lt.s32	%p2, %r2, %r9;
	and.pred  	%p3, %p1, %p2;
	mul.wide.s32 	%rd4, %r19, 4;
	add.s64 	%rd1, %rd3, %rd4;
	@%p3 bra 	BB1_3;
	bra.uni 	BB1_1;

BB1_3:
	cvt.rn.f64.s32	%fd1, %r9;
	rcp.rn.f64 	%fd35, %fd18;
	mul.f64 	%fd19, %fd1, %fd35;
	cvt.rn.f64.s32	%fd3, %r8;
	div.rn.f64 	%fd4, %fd19, %fd3;
	setp.le.s32	%p7, %r9, %r8;
	@%p7 bra 	BB1_4;

	mul.f64 	%fd20, %fd3, %fd35;
	div.rn.f64 	%fd34, %fd20, %fd1;
	bra.uni 	BB1_6;

BB1_1:
	setp.lt.s32	%p4, %r1, %r6;
	setp.lt.s32	%p5, %r2, %r7;
	and.pred  	%p6, %p4, %p5;
	@!%p6 bra 	BB1_13;
	bra.uni 	BB1_2;

BB1_2:
	mov.u16 	%rs7, 0;
	mov.u16 	%rs8, 255;
	st.global.v4.u8 	[%rd1], {%rs7, %rs8, %rs8, %rs7};
	bra.uni 	BB1_13;

BB1_4:
	mov.f64 	%fd34, %fd35;
	mov.f64 	%fd35, %fd4;

BB1_6:
	add.s32 	%r21, %r8, -1;
	cvt.rn.f64.s32	%fd21, %r21;
	add.f64 	%fd22, %fd34, %fd34;
	div.rn.f64 	%fd23, %fd22, %fd21;
	add.s32 	%r22, %r9, -1;
	cvt.rn.f64.s32	%fd24, %r22;
	add.f64 	%fd25, %fd35, %fd35;
	div.rn.f64 	%fd26, %fd25, %fd24;
	cvt.rn.f64.s32	%fd27, %r1;
	sub.f64 	%fd28, %fd16, %fd34;
	fma.rn.f64 	%fd8, %fd27, %fd23, %fd28;
	cvt.rn.f64.s32	%fd29, %r2;
	sub.f64 	%fd30, %fd17, %fd35;
	fma.rn.f64 	%fd9, %fd29, %fd26, %fd30;
	setp.eq.s32	%p8, %r10, 0;
	mov.u32 	%r24, 0;
	shl.b32 	%r3, %r10, 8;
	mov.u16 	%rs11, 0;
	mov.u16 	%rs16, %rs11;
	mov.u16 	%rs17, %rs11;
	mov.u16 	%rs18, %rs11;
	@%p8 bra 	BB1_12;

	mov.f64 	%fd36, %fd9;
	mov.f64 	%fd37, %fd8;

BB1_8:
	mul.f64 	%fd12, %fd36, %fd36;
	mul.f64 	%fd13, %fd37, %fd37;
	add.f64 	%fd31, %fd13, %fd12;
	setp.gt.f64	%p9, %fd31, 0d4010000000000000;
	@%p9 bra 	BB1_11;

	add.f64 	%fd32, %fd37, %fd37;
	fma.rn.f64 	%fd36, %fd32, %fd36, %fd9;
	sub.f64 	%fd33, %fd13, %fd12;
	add.f64 	%fd37, %fd8, %fd33;
	add.s32 	%r24, %r24, 1;
	setp.lt.u32	%p10, %r24, %r3;
	@%p10 bra 	BB1_8;

	mov.u16 	%rs16, %rs11;
	mov.u16 	%rs17, %rs11;
	mov.u16 	%rs18, %rs11;
	bra.uni 	BB1_12;

BB1_11:
	and.b32  	%r23, %r24, 255;
	mul.wide.u32 	%rd5, %r23, 3;
	mov.u64 	%rd6, viridis;
	add.s64 	%rd7, %rd6, %rd5;
	ld.global.u8 	%rs18, [%rd7];
	ld.global.u8 	%rs17, [%rd7+1];
	ld.global.u8 	%rs16, [%rd7+2];

BB1_12:
	st.global.v4.u8 	[%rd1], {%rs18, %rs17, %rs16, %rs11};

BB1_13:
	ret;
}


