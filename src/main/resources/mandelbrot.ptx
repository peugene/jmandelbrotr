//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24817639
// Cuda compilation tools, release 10.0, V10.0.130
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_30
.address_size 64

	// .globl	mandel_float
.global .align 1 .b8 viridis[768] = {84, 1, 68, 85, 2, 68, 87, 3, 69, 88, 5, 69, 90, 6, 69, 91, 8, 70, 93, 9, 70, 94, 11, 70, 96, 12, 70, 97, 14, 71, 98, 15, 71, 100, 17, 71, 101, 18, 71, 102, 20, 71, 104, 21, 72, 105, 22, 72, 106, 24, 72, 108, 25, 72, 109, 26, 72, 110, 28, 72, 111, 29, 72, 112, 30, 72, 113, 32, 72, 115, 33, 72, 116, 34, 72, 117, 36, 72, 118, 37, 72, 119, 38, 72, 120, 39, 72, 121, 41, 71, 121, 42, 71, 122, 43, 71, 123, 44, 71, 124, 46, 71, 125, 47, 70, 126, 48, 70, 126, 49, 70, 127, 51, 70, 128, 52, 69, 129, 53, 69, 129, 54, 69, 130, 56, 68, 131, 57, 68, 131, 58, 68, 132, 59, 67, 132, 60, 67, 133, 62, 67, 133, 63, 66, 134, 64, 66, 134, 65, 65, 135, 66, 65, 135, 67, 65, 136, 69, 64, 136, 70, 64, 136, 71, 63, 137, 72, 63, 137, 73, 62, 137, 74, 62, 138, 75, 61, 138, 77, 61, 138, 78, 60, 138, 79, 60, 139, 80, 59, 139, 81, 59, 139, 82, 58, 139, 83, 58, 140, 84, 57, 140, 85, 57, 140, 86, 56, 140, 87, 56, 140, 88, 55, 140, 89, 55, 141, 91, 54, 141, 92, 54, 141, 93, 53, 141, 94, 53, 141, 95, 52, 141, 96, 52, 141, 97, 51, 141, 98, 51, 141, 99, 51, 142, 100, 50, 142, 101, 50, 142, 102, 49, 142, 103, 49, 142, 104, 48, 142, 105, 48, 142, 106, 47, 142, 107, 47, 142, 108, 47, 142, 109, 46, 142, 110, 46, 142, 111, 45, 142, 112, 45, 142, 112, 45, 142, 113, 44, 142, 114, 44, 142, 115, 43, 142, 116, 43, 142, 117, 43, 142, 118, 42, 142, 119, 42, 142, 120, 41, 142, 121, 41, 142, 122, 41, 142, 123, 40, 142, 124, 40, 142, 125, 40, 142, 126, 39, 142, 127, 39, 142, 128, 38, 142, 129, 38, 142, 130, 38, 142, 130, 37, 142, 131, 37, 142, 132, 37, 142, 133, 36, 142, 134, 36, 142, 135, 35, 142, 136, 35, 142, 137, 35, 141, 138, 34, 141, 139, 34, 141, 140, 34, 141, 141, 33, 141, 142, 33, 141, 143, 33, 141, 144, 32, 140, 145, 32, 140, 146, 32, 140, 147, 32, 140, 147, 31, 140, 148, 31, 139, 149, 31, 139, 150, 31, 139, 151, 31, 139, 152, 30, 138, 153, 30, 138, 154, 30, 138, 155, 30, 137, 156, 30, 137, 157, 30, 137, 158, 30, 136, 159, 30, 136, 160, 30, 136, 161, 31, 135, 162, 31, 135, 163, 31, 134, 163, 31, 134, 164, 32, 134, 165, 32, 133, 166, 33, 133, 167, 33, 132, 168, 34, 131, 169, 35, 131, 170, 35, 130, 171, 36, 130, 172, 37, 129, 173, 38, 129, 174, 39, 128, 175, 40, 127, 175, 41, 127, 176, 42, 126, 177, 43, 125, 178, 44, 124, 179, 46, 124, 180, 47, 123, 181, 48, 122, 182, 50, 121, 183, 51, 121, 183, 53, 120, 184, 54, 119, 185, 56, 118, 186, 57, 117, 187, 59, 116, 188, 61, 115, 189, 62, 114, 189, 64, 113, 190, 66, 112, 191, 68, 111, 192, 70, 110, 193, 72, 109, 194, 73, 108, 194, 75, 107, 195, 77, 106, 196, 79, 105, 197, 81, 104, 198, 83, 102, 198, 85, 101, 199, 88, 100, 200, 90, 99, 201, 92, 98, 201, 94, 96, 202, 96, 95, 203, 98, 94, 204, 101, 92, 204, 103, 91, 205, 105, 90, 206, 108, 88, 206, 110, 87, 207, 112, 85, 208, 115, 84, 208, 117, 82, 209, 119, 81, 210, 122, 79, 210, 124, 78, 211, 127, 76, 212, 129, 75, 212, 132, 73, 213, 134, 72, 213, 137, 70, 214, 139, 68, 215, 142, 67, 215, 144, 65, 216, 147, 63, 216, 149, 62, 217, 152, 60, 217, 155, 58, 218, 157, 57, 218, 160, 55, 219, 163, 53, 219, 165, 51, 220, 168, 50, 220, 171, 48, 221, 173, 46, 221, 176, 45, 221, 179, 43, 222, 181, 41, 222, 184, 39, 223, 187, 38, 223, 189, 36, 223, 192, 35, 224, 195, 33, 224, 197, 32, 225, 200, 30, 225, 203, 29, 225, 205, 28, 226, 208, 27, 226, 211, 26, 226, 213, 25, 227, 216, 24, 227, 219, 24, 227, 221, 24, 228, 224, 24, 228, 226, 24, 228, 229, 25, 229, 232, 25, 229, 234, 26, 229, 237, 27, 230, 239, 28, 230, 242, 30, 230, 244, 31, 230, 247, 33, 231, 249, 35, 231, 251, 36, 231, 254};

.visible .entry mandel_float(
	.param .u64 mandel_float_param_0,
	.param .u32 mandel_float_param_1,
	.param .u32 mandel_float_param_2,
	.param .f32 mandel_float_param_3,
	.param .f32 mandel_float_param_4,
	.param .f32 mandel_float_param_5,
	.param .u32 mandel_float_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<17>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [mandel_float_param_0];
	ld.param.u32 	%r8, [mandel_float_param_1];
	ld.param.u32 	%r9, [mandel_float_param_2];
	ld.param.f32 	%f9, [mandel_float_param_3];
	ld.param.f32 	%f10, [mandel_float_param_4];
	ld.param.f32 	%f11, [mandel_float_param_5];
	ld.param.u32 	%r10, [mandel_float_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r2, %r1, %r11, %r12;
	mov.u32 	%r13, %ntid.y;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.y;
	mad.lo.s32 	%r3, %r13, %r14, %r15;
	setp.lt.s32	%p1, %r2, %r8;
	setp.lt.s32	%p2, %r3, %r9;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB0_8;
	bra.uni 	BB0_1;

BB0_1:
	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r1;
	mad.lo.s32 	%r4, %r18, %r3, %r2;
	shl.b32 	%r5, %r10, 8;
	rcp.rn.f32 	%f12, %f11;
	sub.f32 	%f13, %f9, %f12;
	sub.f32 	%f14, %f10, %f12;
	add.f32 	%f15, %f12, %f12;
	add.s32 	%r19, %r8, -1;
	cvt.rn.f32.s32	%f16, %r19;
	div.rn.f32 	%f17, %f15, %f16;
	add.s32 	%r20, %r9, -1;
	cvt.rn.f32.s32	%f18, %r20;
	div.rn.f32 	%f19, %f15, %f18;
	cvt.rn.f32.s32	%f20, %r2;
	fma.rn.f32 	%f1, %f20, %f17, %f13;
	cvt.rn.f32.s32	%f21, %r3;
	fma.rn.f32 	%f2, %f21, %f19, %f14;
	setp.eq.s32	%p4, %r10, 0;
	mov.u32 	%r22, 0;
	mov.u16 	%rs9, 0;
	mov.u16 	%rs14, %rs9;
	mov.u16 	%rs15, %rs9;
	mov.u16 	%rs16, %rs9;
	@%p4 bra 	BB0_7;

	mov.f32 	%f25, %f2;
	mov.f32 	%f26, %f1;

BB0_3:
	mul.f32 	%f5, %f25, %f25;
	mul.f32 	%f6, %f26, %f26;
	add.f32 	%f22, %f6, %f5;
	setp.gt.f32	%p5, %f22, 0f40800000;
	@%p5 bra 	BB0_6;

	add.f32 	%f23, %f26, %f26;
	fma.rn.f32 	%f25, %f23, %f25, %f2;
	sub.f32 	%f24, %f6, %f5;
	add.f32 	%f26, %f1, %f24;
	add.s32 	%r22, %r22, 1;
	setp.lt.u32	%p6, %r22, %r5;
	@%p6 bra 	BB0_3;

	mov.u16 	%rs14, %rs9;
	mov.u16 	%rs15, %rs9;
	mov.u16 	%rs16, %rs9;
	bra.uni 	BB0_7;

BB0_6:
	and.b32  	%r21, %r22, 255;
	mul.wide.u32 	%rd2, %r21, 3;
	mov.u64 	%rd3, viridis;
	add.s64 	%rd4, %rd3, %rd2;
	ld.global.u8 	%rs16, [%rd4];
	ld.global.u8 	%rs15, [%rd4+1];
	ld.global.u8 	%rs14, [%rd4+2];

BB0_7:
	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r4, 4;
	add.s64 	%rd7, %rd5, %rd6;
	st.global.v4.u8 	[%rd7], {%rs16, %rs15, %rs14, %rs9};

BB0_8:
	ret;
}

	// .globl	mandel_double
.visible .entry mandel_double(
	.param .u64 mandel_double_param_0,
	.param .u32 mandel_double_param_1,
	.param .u32 mandel_double_param_2,
	.param .f64 mandel_double_param_3,
	.param .f64 mandel_double_param_4,
	.param .f64 mandel_double_param_5,
	.param .u32 mandel_double_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<23>;
	.reg .f64 	%fd<27>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [mandel_double_param_0];
	ld.param.u32 	%r8, [mandel_double_param_1];
	ld.param.u32 	%r9, [mandel_double_param_2];
	ld.param.f64 	%fd9, [mandel_double_param_3];
	ld.param.f64 	%fd10, [mandel_double_param_4];
	ld.param.f64 	%fd11, [mandel_double_param_5];
	ld.param.u32 	%r10, [mandel_double_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r2, %r1, %r11, %r12;
	mov.u32 	%r13, %ntid.y;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %tid.y;
	mad.lo.s32 	%r3, %r13, %r14, %r15;
	setp.lt.s32	%p1, %r2, %r8;
	setp.lt.s32	%p2, %r3, %r9;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB1_8;
	bra.uni 	BB1_1;

BB1_1:
	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r18, %r17, %r1;
	mad.lo.s32 	%r4, %r18, %r3, %r2;
	shl.b32 	%r5, %r10, 8;
	rcp.rn.f64 	%fd12, %fd11;
	sub.f64 	%fd13, %fd9, %fd12;
	sub.f64 	%fd14, %fd10, %fd12;
	add.f64 	%fd15, %fd12, %fd12;
	add.s32 	%r19, %r8, -1;
	cvt.rn.f64.s32	%fd16, %r19;
	div.rn.f64 	%fd17, %fd15, %fd16;
	add.s32 	%r20, %r9, -1;
	cvt.rn.f64.s32	%fd18, %r20;
	div.rn.f64 	%fd19, %fd15, %fd18;
	cvt.rn.f64.s32	%fd20, %r2;
	fma.rn.f64 	%fd1, %fd20, %fd17, %fd13;
	cvt.rn.f64.s32	%fd21, %r3;
	fma.rn.f64 	%fd2, %fd21, %fd19, %fd14;
	setp.eq.s32	%p4, %r10, 0;
	mov.u32 	%r22, 0;
	mov.u16 	%rs9, 0;
	mov.u16 	%rs14, %rs9;
	mov.u16 	%rs15, %rs9;
	mov.u16 	%rs16, %rs9;
	@%p4 bra 	BB1_7;

	mov.f64 	%fd25, %fd2;
	mov.f64 	%fd26, %fd1;

BB1_3:
	mul.f64 	%fd5, %fd25, %fd25;
	mul.f64 	%fd6, %fd26, %fd26;
	add.f64 	%fd22, %fd6, %fd5;
	setp.gt.f64	%p5, %fd22, 0d4010000000000000;
	@%p5 bra 	BB1_6;

	add.f64 	%fd23, %fd26, %fd26;
	fma.rn.f64 	%fd25, %fd23, %fd25, %fd2;
	sub.f64 	%fd24, %fd6, %fd5;
	add.f64 	%fd26, %fd1, %fd24;
	add.s32 	%r22, %r22, 1;
	setp.lt.u32	%p6, %r22, %r5;
	@%p6 bra 	BB1_3;

	mov.u16 	%rs14, %rs9;
	mov.u16 	%rs15, %rs9;
	mov.u16 	%rs16, %rs9;
	bra.uni 	BB1_7;

BB1_6:
	and.b32  	%r21, %r22, 255;
	mul.wide.u32 	%rd2, %r21, 3;
	mov.u64 	%rd3, viridis;
	add.s64 	%rd4, %rd3, %rd2;
	ld.global.u8 	%rs16, [%rd4];
	ld.global.u8 	%rs15, [%rd4+1];
	ld.global.u8 	%rs14, [%rd4+2];

BB1_7:
	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r4, 4;
	add.s64 	%rd7, %rd5, %rd6;
	st.global.v4.u8 	[%rd7], {%rs16, %rs15, %rs14, %rs9};

BB1_8:
	ret;
}


